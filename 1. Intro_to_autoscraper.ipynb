{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : to scrape the information about the github repositories using autoscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Auto scrapper is a libraray built for scarpping of websites and makes the process very simle and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoscraper\n",
      "  Downloading autoscraper-1.1.12-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from autoscraper) (2.18.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from autoscraper) (4.6.1)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from bs4->autoscraper) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4->autoscraper) (2.0.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->autoscraper) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->autoscraper) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->autoscraper) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\anaconda\\lib\\site-packages (from requests->autoscraper) (2020.11.8)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1283 sha256=f587b4dc2a9598dcafa8ab336d8b65aaa08fa6f79cdf3180797f79299cd914ee\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\19\\f5\\6d\\a97dd4f22376d4472d5f4c76c7646876052ff3166b3cf71050\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4, autoscraper\n",
      "Successfully installed autoscraper-1.1.12 bs4-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\dell\\anaconda3\\anaconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "## Insalling the required library.\n",
    "!pip install autoscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoscraper import AutoScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cyber-Security-Prediction', 'Breast-Cancer-Analysis', 'Hands-on-Machine-learning-with-Scikit-learn-Tensorflow-and-Keras\\n\\n\\n          Forked from Py-Contributors/Hands-on-Machine-learning-with-Scikit-learn-Tensorflow-and-Keras', 'The-Layman-Machine-Learning-dictionary\\n\\n\\n          Forked from vkassingh/The-Layman-Machine-Learning-dictionary', 'Hacktoberfest2020\\n\\n\\n          Forked from Cullyege/Hacktoberfest2020', 'opencv\\n\\n\\n          Forked from opencv/opencv', 'Auto-mpg', 'Personalized-cancer-diagnosis', 'car-safety', 'Churn-prediction']\n"
     ]
    }
   ],
   "source": [
    "## now take a url of the page tha you want to scrape and create a list of the wanted items from the page \n",
    "url = 'https://github.com/Sahil-Gupta-ml?tab=repositories'\n",
    "\n",
    "## in the wanated list more than one item can also be added.\n",
    "wanted_list = [\"Cyber-Security-Prediction\"]\n",
    "\n",
    "scraper = AutoScraper()\n",
    "result = scraper.build(url, wanted_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can be seen that all the repository names have been scraped from the github page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* so to build the scraper in sucha way that it can extract same iniformation from other pages to we need to perform grouping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rule_xl7r': ['finbert_embedding',\n",
       "  'bert-as-service\\n\\n\\n          Forked from hanxiao/bert-as-service',\n",
       "  'bert-embedding\\n\\n\\n          Forked from imgarylai/bert-embedding',\n",
       "  'Speaker-identification-using-GMMs',\n",
       "  'Food-Classification-Task-Transfer-learning',\n",
       "  'Web-Crawling-Scrapping-Bot',\n",
       "  'opencv\\n\\n\\n          Forked from opencv/opencv',\n",
       "  'QA-system-memory-networks-bAbI-dataset',\n",
       "  'NER_Disease_Extraction_Hackathon',\n",
       "  'Predict-the-Happiness-HackerEarth-Challenge',\n",
       "  'Toxic_Comment_Classification_Challenge',\n",
       "  'Change-Detection-in-Satellite-Imagery',\n",
       "  'tf-fashionMNIST',\n",
       "  'FaceEmotion_ID',\n",
       "  'Topic-Modelling-on-Wiki-corpus',\n",
       "  'Face_ID',\n",
       "  'Image-compression-with-Kmeans-clustering',\n",
       "  'WNS-DS-Hackathon',\n",
       "  'Object-recognition-CIFAR-10',\n",
       "  'darknet\\n\\n\\n          Forked from pjreddie/darknet',\n",
       "  'Text-classification-and-clustering',\n",
       "  'User-Verification-based-on-Keystroke-Dynamics',\n",
       "  'PyGender-Voice',\n",
       "  'Language-Detection-From-Text---Bi-gram-based',\n",
       "  'Sentiment-Analysis-using-tf-idf---Polarity-dataset',\n",
       "  'Mail-Spam-Filtering']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.get_result_similar(\"https://github.com/abhijeet3922?tab=repositories\", grouped = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result came out and is precise but it is showing under some rule and we need to change tha soo th enext step\n",
    "scraper.set_rule_aliases({'rule_xl7r': \"Repo_name\"})\n",
    "scraper.keep_rules(['rule_xl7r'])\n",
    "\n",
    "# saving the scraper for future use.\n",
    "scraper.save('github-repo_name-extractor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the scraper\n",
    "scraper.load(\"github-repo_name-extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = scraper.get_result_similar('https://github.com/abhijeet3922?tab=repositories', group_by_alias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finbert_embedding',\n",
       " 'bert-as-service\\n\\n\\n          Forked from hanxiao/bert-as-service',\n",
       " 'bert-embedding\\n\\n\\n          Forked from imgarylai/bert-embedding',\n",
       " 'Speaker-identification-using-GMMs',\n",
       " 'Food-Classification-Task-Transfer-learning',\n",
       " 'Web-Crawling-Scrapping-Bot',\n",
       " 'opencv\\n\\n\\n          Forked from opencv/opencv',\n",
       " 'QA-system-memory-networks-bAbI-dataset',\n",
       " 'NER_Disease_Extraction_Hackathon',\n",
       " 'Predict-the-Happiness-HackerEarth-Challenge',\n",
       " 'Toxic_Comment_Classification_Challenge',\n",
       " 'Change-Detection-in-Satellite-Imagery',\n",
       " 'tf-fashionMNIST',\n",
       " 'FaceEmotion_ID',\n",
       " 'Topic-Modelling-on-Wiki-corpus',\n",
       " 'Face_ID',\n",
       " 'Image-compression-with-Kmeans-clustering',\n",
       " 'WNS-DS-Hackathon',\n",
       " 'Object-recognition-CIFAR-10',\n",
       " 'darknet\\n\\n\\n          Forked from pjreddie/darknet',\n",
       " 'Text-classification-and-clustering',\n",
       " 'User-Verification-based-on-Keystroke-Dynamics',\n",
       " 'PyGender-Voice',\n",
       " 'Language-Detection-From-Text---Bi-gram-based',\n",
       " 'Sentiment-Analysis-using-tf-idf---Polarity-dataset',\n",
       " 'Mail-Spam-Filtering']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"Repo_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So saving and loading the model is working just fine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
